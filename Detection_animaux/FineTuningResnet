import os
import cv2
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import Callback
import matplotlib.pyplot as plt


# Définir le chemin vers votre ensemble de données pour le fine-tuning
train_data_dir = '/Volumes/Expansion/classes_train3000'
validation_data_dir = '/Volumes/Expansion/classes_test3000'

# Liste des classes (noms des sous-répertoires)
classes = sorted(os.listdir(train_data_dir))

import cv2
import os
import numpy as np

def load_and_resize_images(directory, target_size=((500), 500)):
    images = []
    labels = []
    for i, classe in enumerate(classes):
        classe_path = os.path.join(directory, classe)
        for fichier_image in os.listdir(classe_path):
            chemin_image = os.path.join(classe_path, fichier_image)
            image = cv2.imread(chemin_image)

            # Vérifier si l'image a été correctement chargée
            if image is not None:
                # Redimensionner l'image
                image = cv2.resize(image, target_size)

                images.append(image)
                labels.append(i)

    return np.array(images), np.array(labels)
def load_and_resize_images2(directory, target_size=(500, 500)):
    images = []
    labels = []
    for i, classe in enumerate(classes):
        classe_path = os.path.join(directory, classe)

        for fichier_image in os.listdir(classe_path):
            chemin_image = os.path.join(classe_path, fichier_image)
            image = cv2.imread(chemin_image)
            # Vérifier si l'image a été correctement chargée
            if image is not None:
                # Trouver la dimension maximale
                max_dim = max(image.shape[0], image.shape[1])

                # Calculer les bordures nécessaires pour rendre l'image carrée
                pad_top = (max_dim - image.shape[0]) // 2
                pad_bottom = max_dim - image.shape[0] - pad_top
                pad_left = (max_dim - image.shape[1]) // 2
                pad_right = max_dim - image.shape[1] - pad_left

                # Ajouter des bordures vides
                image = cv2.copyMakeBorder(image, pad_top, pad_bottom, pad_left, pad_right, cv2.BORDER_CONSTANT, value=(0, 0, 0))

                # Redimensionner l'image
                image = cv2.resize(image, target_size)
                
                images.append(image)
                labels.append(i)

        

    return np.array(images), np.array(labels)

# Charger les données d'entraînement et de validation
train_data, train_labels = load_and_resize_images2(train_data_dir)
validation_data, validation_labels = load_and_resize_images2(validation_data_dir)
print('data ok')
# Convertir les labels en encodage one-hot
train_labels_onehot = tf.keras.utils.to_categorical(train_labels, num_classes=len(classes))
validation_labels_onehot = tf.keras.utils.to_categorical(validation_labels, num_classes=len(classes))

# Charger le modèle ResNet50 pré-entraîné (sans les couches fully-connected)
base_model = ResNet50(weights='imagenet', include_top=False)

# Ajouter une couche GlobalAveragePooling2D pour réduire le nombre de paramètres
x = base_model.output
x = GlobalAveragePooling2D()(x)

# Ajouter une couche Dense avec le nombre de classes pour la classification
predictions = Dense(len(classes), activation='softmax')(x)

# Créer le modèle final
model = Model(inputs=base_model.input, outputs=predictions)

# Geler les poids du modèle pré-entraîné
for layer in base_model.layers:
    layer.trainable = False

# Compiler le modèle
model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

# Définir un callback pour afficher les données d'entraînement
class TrainingDataCallback(Callback):
    def __init__(self):
        self.losses = []
        self.accuracies = []
        self.val_losses = []
        self.val_accuracies = []

    def on_epoch_end(self, epoch, logs=None):
        self.losses.append(logs['loss'])
        self.accuracies.append(logs['accuracy'])
        self.val_losses.append(logs['val_loss'])
        self.val_accuracies.append(logs['val_accuracy'])
        print(f"Epoch {epoch + 1}/{self.params['epochs']} - Loss: {logs['loss']:.4f} - Accuracy: {logs['accuracy']:.4f} - Validation Loss: {logs['val_loss']:.4f} - Validation Accuracy: {logs['val_accuracy']:.4f}")

# Entraînement du modèle
history = model.fit(train_data, train_labels_onehot, epochs=20, validation_data=(validation_data, validation_labels_onehot), batch_size=32)

# Visualisation de l'erreur d'entraînement et de validation
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.show()
plt.plot(history.history['val_accuracy'], label='Validation accuracy')
plt.plot(history.history['accuracy'], label='Training accuracy')
plt.legend()
plt.plot()
